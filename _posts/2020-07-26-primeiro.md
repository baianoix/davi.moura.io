---
layout: post
title: "Introdução"
author: "Davi Moura"
categories: facts
tags: [sample]
image: download.jpg
---

Segundo a [Wikipédia](https://en.wikipedia.org/wiki/Econometrics)  econometria é a aplicação de métodos estatísticos em dados econômicos para dar conteúdo empírico para as relações econômicas.

Assim começaremos vendo as regressões lineares e ver algumas aplicações!(Do [wooldridge](https://www.amazon.com.br/Introdu%C3%A7%C3%A3o-%C3%A0-econometria-abordagem-moderna/dp/8522125643/ref=sr_1_1?__mk_pt_BR=%C3%85M%C3%85%C5%BD%C3%95%C3%91&crid=1PM8D1E0KQIYC&dchild=1&keywords=wooldridge&qid=1595773140&sprefix=wooldri%2Caps%2C306&sr=8-1) mesmo). Lembramos então das suposições que são feitas para que os estimadores do OLS sejam **não viesados** e **consistentes**.

$$ E(\hat{u}|X)=0  $$

Primeiro consideramos que há exogeneidade das variáveis explicativas com relação a û, claro que é uma suposição muito forte, mas devemos começar por aí. Ademais, queremos que os estimadores sejam consistentes, ou seja, dado que a amostra tende a infinito, o estimador aproxima-se ao valor verdadeiro $\beta$


$$plim(\hat{\beta})=\beta$$


Começando com a implementação em R. Para os códigos nesse artigo utilizaremos apenas as bibliotecas do wooldridge e o lmtest. Então, escreva em seu console:
install.packages("wooldridge") e install.packages("lmtest") - Não esqueça de colocar as aspas.

### Exemplo 3.1

``` r
library(wooldridge)


reg<- lm(colGPA~hsGPA+ACT,data=gpa1)
summary(reg)
```

Então, para escrever uma regressão do R deveremos colocar lm, que é o modelo linear (**linear model**), a variável dependente, seguida por um "~" e depois a soma das variáveis explicativas. Além disso colocamos qual é a base de dados, que nesse caso é "gpa1" depois da equação.

### Exemplo 3.5

``` r
library(wooldridge)


reg<- lm(narr86~pcnv+ptime86+qemp86,data=crime1)
summary(reg)
```

## Viés de variável omitida

Suponha que, no exemplo 3.1, tiramos o GPA do ensino médio da regressão, o que acontecerá? A equação é dada por:

$$ y = \beta_0 + \beta_1 x_1+ \beta_2 x_2 $$

Para facilitar notação colocamos $x_2$ como *hsGPA*, temos então:

$$ E(\tilde{\beta_1})= E(\hat{\beta_1} + \beta_2\tilde{\delta}) $$

Logo, o viés sobre $ \beta_1 $ depende do próprio valor de $ \beta_2 $ e do valor da correlação entre os betas, lembrando que o viés de um estimador é calculado por:

$$ Viés(\theta) = E(\hat{\theta}) - \theta $$

Temos, que:
    \ |  Corr(x1,x2)>0|Corr(x1,x2)>0|
-------|-----------|-----------|
$ \beta_2 >0 $     | Viés Positivo | Viés Negativo|
$ \beta_2 <0 $    | Viés Negativo| Viés Positivo|
